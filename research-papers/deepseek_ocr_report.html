<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DeepSeek-OCR: Contexts Optical Compression</title>
    <style>
        @page {
            margin: 2cm;
        }
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: white;
        }
        h1 {
            color: #1a1a1a;
            font-size: 24px;
            margin-bottom: 10px;
            text-align: center;
            border-bottom: 2px solid #DC8850;
            padding-bottom: 15px;
        }
        h2 {
            color: #DC8850;
            font-size: 20px;
            margin-top: 30px;
            margin-bottom: 15px;
            border-bottom: 1px solid #e0e0e0;
            padding-bottom: 8px;
        }
        h3 {
            color: #555;
            font-size: 16px;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        .authors {
            text-align: center;
            font-style: italic;
            margin-bottom: 30px;
            color: #666;
        }
        .abstract {
            background: #f8f8f8;
            padding: 20px;
            border-left: 4px solid #DC8850;
            margin: 20px 0;
        }
        .key-finding {
            background: #fff8f0;
            padding: 15px;
            border-left: 4px solid #DC8850;
            margin: 15px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #DC8850;
            color: white;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        .metric {
            font-weight: bold;
            color: #DC8850;
        }
        .performance-improvement {
            color: #27ae60;
            font-weight: bold;
        }
        .performance-decline {
            color: #e74c3c;
            font-weight: bold;
        }
        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        li {
            margin: 8px 0;
        }
        .methodology-box {
            background: #f0f8ff;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
        }
        .conclusion-box {
            background: #f0f0f0;
            padding: 20px;
            border-radius: 5px;
            margin-top: 30px;
        }
        .badge {
            display: inline-block;
            padding: 4px 10px;
            background: #DC8850;
            color: white;
            border-radius: 3px;
            font-size: 12px;
            font-weight: bold;
            margin-right: 8px;
        }
        .formula {
            background: #f5f5f5;
            padding: 10px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            text-align: center;
            margin: 15px 0;
        }
        .eli5-box {
            background: #e8f5e9;
            padding: 20px;
            border-left: 4px solid #4caf50;
            margin: 20px 0;
            font-size: 15px;
        }
        .navigation {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background: #f8f8f8;
            border-radius: 5px;
        }
        .navigation a {
            color: #DC8850;
            text-decoration: none;
            margin: 0 15px;
            font-weight: bold;
        }
        .navigation a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="navigation">
        <a href="../index.html">‚Üê Home</a>
        <a href="../agent/index.html">Agent Reliability</a>
        <a href="../rag/index.html">RAG Patterns</a>
        <a href="../research-papers/index.html">Research Papers</a>
        <a href="https://join.maxpool.dev" target="_blank">Join Community ‚Üí</a>
    </div>

    <h1>DeepSeek-OCR: Contexts Optical Compression</h1>

    <div class="authors">
        Haoran Wei, Yaofeng Sun, Yukun Li<br>
        <em>DeepSeek-AI</em>
    </div>

    <div class="abstract">
        <h2>Executive Summary</h2>
        <p>DeepSeek-OCR represents a paradigm shift in how AI systems handle long text contexts by treating vision as a compression medium. Rather than processing text token-by-token, the system converts documents into images and compresses them 7-20√ó while maintaining high accuracy. At 10√ó compression, the model achieves 97% OCR precision; even at 20√ó compression, accuracy remains around 60%. This approach directly addresses the quadratic computational scaling problem in large language models when processing long contexts. Beyond OCR performance, DeepSeek-OCR demonstrates a novel path toward implementing memory forgetting mechanisms in AI systems‚Äîolder conversation rounds can be stored at progressively lower resolutions, mirroring human memory decay. With production throughput of 200,000+ pages per day on a single A100 GPU, the system has immediate practical value while opening research directions for vision-text compression, context management, and multimodal architecture design.</p>
    </div>
        <div class="video-overview">
            <h2>Video Overview</h2>
            <p>
                <a href="https://youtu.be/bcXdp58itvk" target="_blank">Watch the video overview on YouTube</a>
            </p>
        </div>

    <div class="eli5-box">
        <h2>üßí ELI5: The Core Idea</h2>
        <p><strong>Imagine your brain trying to remember a long conversation...</strong></p>
        <p>Right now, AI chatbots are like someone with a perfect memory who has to read through <em>every single word</em> of a conversation from the beginning every time you say something new. If you've been chatting for an hour, that's thousands of words to re-read!</p>
        <p><strong>DeepSeek-OCR has a clever trick: it takes a picture of old messages.</strong></p>
        <p>Think about it‚Äîif I show you a photo of a page from a book, you can see all the words at once. The photo file is much smaller than storing each letter separately. DeepSeek-OCR does the same thing: it converts old text into images, and those images take up way less "brain space" (tokens) for the AI to remember.</p>
        <p><strong>Even better: it mimics how human memory works!</strong></p>
        <ul>
            <li><strong>Recent memories (1 minute ago):</strong> Crystal clear, high resolution ‚Üí keeps all the details</li>
            <li><strong>Older memories (1 hour ago):</strong> A bit fuzzy ‚Üí uses a smaller, blurrier image</li>
            <li><strong>Ancient memories (last week):</strong> Very blurry ‚Üí tiny, low-quality image with just the gist</li>
        </ul>
        <p>This lets the AI "remember" 10 times more conversation history while using the same amount of brain power! It's like being able to fit 10 books into your backpack by taking pictures of the pages instead of carrying the actual books.</p>
    </div>

    <h2>Research Context & Motivation</h2>
    
    <h3>The Long Context Problem</h3>
    <p>Large language models face a fundamental computational bottleneck: processing cost scales quadratically with context length. When a document contains 10,000 tokens, processing requires managing 10,000¬≤ = 100 million interactions in self-attention mechanisms. This becomes prohibitively expensive for:</p>
    <ul>
        <li><strong>Multi-turn conversations:</strong> Each response requires reprocessing the entire history</li>
        <li><strong>Long documents:</strong> Academic papers, legal documents, books require excessive compute</li>
        <li><strong>Agent systems:</strong> Persistent agents accumulating interaction histories over time</li>
    </ul>

    <h3>The Core Insight: Vision as Compression</h3>
    <div class="key-finding">
        <p><strong>Key Observation:</strong> A single image containing document text can represent rich information using substantially fewer tokens than the equivalent digital text.</p>
        <p>A 1024√ó1024 image rendered from 1,000 text tokens can be encoded into just 100 vision tokens‚Äîa 10√ó compression ratio‚Äîwhile maintaining 97%+ decoding accuracy.</p>
    </div>

    <p>This insight flips the traditional VLM (Vision-Language Model) paradigm. Instead of asking "how can vision encoders help LLMs understand images?", DeepSeek-OCR asks: "how can vision encoders help LLMs process text more efficiently?"</p>

    <h2>Key Contributions</h2>
    <ol>
        <li><strong>Quantitative Vision-Text Compression Analysis:</strong> First comprehensive study demonstrating 7-20√ó text compression via optical mapping with measured accuracy bounds</li>
        <li><strong>DeepEncoder Architecture:</strong> Novel vision encoder maintaining low activation memory and minimal vision tokens under high-resolution inputs through serial connection of window attention and global attention components</li>
        <li><strong>Production-Ready OCR System:</strong> State-of-the-art performance on OmniDocBench using fewer vision tokens than existing models, with 200k+ pages/day throughput on single GPU</li>
        <li><strong>Memory Forgetting Mechanism:</strong> Conceptual framework for implementing progressive context compression mimicking human memory decay</li>
    </ol>

    <h2>Architecture Deep Dive</h2>

    <h3>System Overview</h3>
    <div class="methodology-box">
        <p><span class="badge">TWO-COMPONENT DESIGN</span></p>
        <ul>
            <li><strong>DeepEncoder (380M parameters):</strong> Vision encoder that compresses images into compact token representations</li>
            <li><strong>DeepSeek3B-MoE (570M active):</strong> MoE decoder that reconstructs text from compressed vision tokens</li>
        </ul>
    </div>

    <h3>DeepEncoder: The Core Innovation</h3>
    
    <div class="eli5-box">
        <h3>üéØ Simple Explanation</h3>
        <p><strong>DeepEncoder is like a smart camera with two lenses:</strong></p>
        <ol>
            <li><strong>First lens (SAM - 80M params):</strong> Takes quick, detailed snapshots of small areas. It looks at little windows of the image, one piece at a time. This is fast and doesn't use much memory because it only looks at small chunks.</li>
            <li><strong>Squisher in the middle (16√ó compressor):</strong> Takes all those little snapshots and squishes them down. Like when you zip a file on your computer‚Äîsame information, much smaller size.</li>
            <li><strong>Second lens (CLIP - 300M params):</strong> Looks at the whole squished-down picture at once to understand the big picture and add "knowledge" about what things mean.</li>
        </ol>
        <p><strong>The genius:</strong> By doing the hard work (looking at details) when things are split into small pieces, and only looking at everything together AFTER it's been squished, it stays fast and doesn't run out of memory!</p>
    </div>

    <h3>DeepEncoder Technical Architecture</h3>
    
    <div class="methodology-box">
        <h4>Design Requirements</h4>
        <ol>
            <li>Process high resolutions (up to 1280√ó1280)</li>
            <li>Maintain low activation memory</li>
            <li>Generate few vision tokens</li>
            <li>Support multiple resolution inputs</li>
            <li>Moderate parameter count (fit on single GPU)</li>
        </ol>

        <h4>Three-Stage Pipeline</h4>
        
        <p><span class="badge">STAGE 1: LOCAL PERCEPTION</span></p>
        <ul>
            <li><strong>Component:</strong> SAM-base (80M parameters, patch size 16)</li>
            <li><strong>Mechanism:</strong> Window attention processing local image regions</li>
            <li><strong>Input:</strong> 1024√ó1024 image ‚Üí 4,096 patch tokens (64√ó64 grid)</li>
            <li><strong>Benefit:</strong> Window attention keeps activation memory manageable even with 4k tokens</li>
        </ul>

        <p><span class="badge">STAGE 2: TOKEN COMPRESSION</span></p>
        <ul>
            <li><strong>Component:</strong> 2-layer convolutional module</li>
            <li><strong>Mechanism:</strong> 16√ó downsampling (kernel=3, stride=2, padding=1)</li>
            <li><strong>Channel progression:</strong> 256 ‚Üí 1024 dimensions</li>
            <li><strong>Output:</strong> 4,096 tokens ‚Üí 256 tokens</li>
            <li><strong>Benefit:</strong> Drastically reduces tokens before expensive global attention</li>
        </ul>

        <p><span class="badge">STAGE 3: GLOBAL CONTEXT</span></p>
        <ul>
            <li><strong>Component:</strong> CLIP-large (300M parameters)</li>
            <li><strong>Mechanism:</strong> Dense global attention over compressed tokens</li>
            <li><strong>Input:</strong> 256 compressed tokens</li>
            <li><strong>Benefit:</strong> Adds pre-trained visual knowledge while operating on manageable token count</li>
        </ul>
    </div>

    <h3>Why This Architecture Wins</h3>
    <table>
        <thead>
            <tr>
                <th>Typical VLM Approach</th>
                <th>Problem</th>
                <th>DeepEncoder Solution</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Tile-based (InternVL)</td>
                <td>Low native resolution ‚Üí excessive fragmentation ‚Üí too many tokens</td>
                <td>High native resolution (1024+) ‚Üí minimal tiling needed</td>
            </tr>
            <tr>
                <td>Adaptive resolution (Qwen2-VL)</td>
                <td>Massive activation memory ‚Üí GPU OOM on large images</td>
                <td>Window attention + compression before global attention</td>
            </tr>
            <tr>
                <td>Dual-tower (Vary)</td>
                <td>Complex preprocessing, hard to parallelize</td>
                <td>Single serial pipeline, simple and efficient</td>
            </tr>
        </tbody>
    </table>

    <h3>Multi-Resolution Support</h3>
    
    <div class="eli5-box">
        <h3>üì∏ Camera Modes Analogy</h3>
        <p><strong>DeepEncoder is like a camera with different quality settings:</strong></p>
        <ul>
            <li><strong>Tiny/Small (64-100 tokens):</strong> Like your phone's "low quality" mode‚Äîgood for quick snapshots, uses little memory</li>
            <li><strong>Base/Large (256-400 tokens):</strong> Like "HD quality"‚Äîcrisp and clear for important documents</li>
            <li><strong>Gundam Mode:</strong> Like panorama mode‚Äîtakes several overlapping photos and a wide-angle shot, then stitches them together for huge documents</li>
        </ul>
        <p>The genius: ONE model can switch between all these modes. You tell it "use 100 tokens" or "use 400 tokens" and it adjusts on the fly!</p>
    </div>

    <table>
        <thead>
            <tr>
                <th>Mode</th>
                <th>Resolution</th>
                <th>Vision Tokens</th>
                <th>Use Case</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Tiny</strong></td>
                <td>512√ó512</td>
                <td>64</td>
                <td>Simple documents, maximum compression</td>
            </tr>
            <tr>
                <td><strong>Small</strong></td>
                <td>640√ó640</td>
                <td>100</td>
                <td>Standard documents, good balance</td>
            </tr>
            <tr>
                <td><strong>Base</strong></td>
                <td>1024√ó1024</td>
                <td>256 (182 valid)</td>
                <td>Detailed documents, preserves aspect ratio</td>
            </tr>
            <tr>
                <td><strong>Large</strong></td>
                <td>1280√ó1280</td>
                <td>400 (285 valid)</td>
                <td>High-quality OCR, complex layouts</td>
            </tr>
            <tr>
                <td><strong>Gundam</strong></td>
                <td>n√ó640 + 1024 global</td>
                <td>n√ó100 + 256</td>
                <td>Newspapers, multi-page documents</td>
            </tr>
            <tr>
                <td><strong>Gundam-Master</strong></td>
                <td>n√ó1024 + 1280 global</td>
                <td>n√ó256 + 400</td>
                <td>Maximum quality, ultra-high resolution</td>
            </tr>
        </tbody>
    </table>

    <h3>The MoE Decoder</h3>
    <p><strong>DeepSeek3B-MoE Architecture:</strong></p>
    <ul>
        <li><strong>Total experts:</strong> 64 routed + 2 shared = 66 expert networks</li>
        <li><strong>Active per token:</strong> 6 routed + 2 shared = 8 experts</li>
        <li><strong>Parameters:</strong> 3B total, 570M activated per forward pass</li>
        <li><strong>Benefit:</strong> Expressive power of 3B model with inference cost of 570M model</li>
    </ul>

    <div class="formula">
        f_decode: R^(n√ód_latent) ‚Üí R^(N√ód_text)<br>
        where n ‚â§ N (compression ratio)<br>
        <br>
        Compressed vision tokens ‚Üí Reconstructed text
    </div>

    <h2>Training Methodology</h2>

    <h3>Data Engine: Comprehensive and Diverse</h3>

    <div class="methodology-box">
        <h4>OCR 1.0 Data (Traditional OCR)</h4>
        <ul>
            <li><strong>PDF Documents:</strong> 30M pages across ~100 languages (25M Chinese/English, 5M others)</li>
            <li><strong>Annotation Types:</strong>
                <ul>
                    <li>Coarse: Direct fitz extraction for text recognition</li>
                    <li>Fine: 2M pages each CN/EN with layout + OCR interleaved format</li>
                    <li>Minority languages: 600K samples using model flywheel (layout model + trained GOT-OCR)</li>
                </ul>
            </li>
            <li><strong>Word Documents:</strong> 3M pages for formulas and HTML tables</li>
            <li><strong>Natural Scene OCR:</strong> 20M images (10M CN, 10M EN) from LAION/Wukong, labeled with PaddleOCR</li>
        </ul>

        <h4>OCR 2.0 Data (Deep Parsing)</h4>
        <ul>
            <li><strong>Charts:</strong> 10M synthetic images (line, bar, pie, composite) rendered with pyecharts/matplotlib ‚Üí HTML table format</li>
            <li><strong>Chemical Formulas:</strong> 5M SMILES from PubChem rendered with RDKit</li>
            <li><strong>Plane Geometry:</strong> 1M synthetic images with translation-invariant augmentation</li>
        </ul>

        <h4>General Vision Data (20% of total)</h4>
        <ul>
            <li>Caption, detection, grounding tasks</li>
            <li>Preserves general VLM interface for future research</li>
        </ul>

        <h4>Text-Only Data (10% of total)</h4>
        <ul>
            <li>In-house pretrain data, 8192 token sequences</li>
            <li>Maintains language capabilities</li>
        </ul>
    </div>

    <h3>Two-Stage Training Pipeline</h3>

    <div class="methodology-box">
        <h4>Stage 1: Training DeepEncoder Independently</h4>
        <ul>
            <li><strong>Setup:</strong> DeepEncoder + compact LM, next-token prediction</li>
            <li><strong>Data:</strong> All OCR 1.0/2.0 + 100M LAION samples</li>
            <li><strong>Config:</strong> 2 epochs, batch size 1280, AdamW optimizer, cosine schedule, LR 5e-5, seq length 4096</li>
        </ul>

        <h4>Stage 2: Training DeepSeek-OCR End-to-End</h4>
        <ul>
            <li><strong>Pipeline Parallelism (PP=4):</strong>
                <ul>
                    <li>PP0: SAM + compressor (frozen, vision tokenizer)</li>
                    <li>PP1: CLIP (unfrozen, trained as input embedding)</li>
                    <li>PP2/PP3: DeepSeek3B-MoE (6 layers each)</li>
                </ul>
            </li>
            <li><strong>Infrastructure:</strong> 20 nodes √ó 8 A100-40G, DP=40, global batch 640</li>
            <li><strong>Config:</strong> AdamW, step-based schedule, initial LR 3e-5</li>
            <li><strong>Throughput:</strong> 90B tokens/day (text-only), 70B tokens/day (multimodal)</li>
        </ul>
    </div>

    <h2>Experimental Results: Compression Study</h2>

    <h3>Vision-Text Compression Bounds</h3>
    
    <div class="key-finding">
        <h4>Tested on Fox Benchmark (100 English pages, 600-1300 tokens)</h4>
        <ul>
            <li><strong>10√ó compression (100 vision tokens):</strong> 89-98% precision across document lengths</li>
            <li><strong>12√ó compression:</strong> ~87% precision</li>
            <li><strong>20√ó compression (64 vision tokens):</strong> 59-96% precision depending on document length</li>
        </ul>
        <p><strong>Key Finding:</strong> Near-lossless compression achievable up to 10√ó ratio. Beyond this, performance degrades but remains useful.</p>
    </div>

    <table>
        <thead>
            <tr>
                <th>Text Tokens (Ground Truth)</th>
                <th>64 Vision Tokens</th>
                <th>Compression</th>
                <th>100 Vision Tokens</th>
                <th>Compression</th>
                <th>Pages</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>600-700</td>
                <td class="metric">96.5%</td>
                <td class="metric">10.5√ó</td>
                <td class="metric">98.5%</td>
                <td class="metric">6.7√ó</td>
                <td>7</td>
            </tr>
            <tr>
                <td>700-800</td>
                <td class="metric">93.8%</td>
                <td class="metric">11.8√ó</td>
                <td class="metric">97.3%</td>
                <td class="metric">7.5√ó</td>
                <td>28</td>
            </tr>
            <tr>
                <td>800-900</td>
                <td class="metric">83.8%</td>
                <td class="metric">13.2√ó</td>
                <td class="metric">96.8%</td>
                <td class="metric">8.5√ó</td>
                <td>28</td>
            </tr>
            <tr>
                <td>900-1000</td>
                <td class="metric">85.9%</td>
                <td class="metric">15.1√ó</td>
                <td class="metric">96.8%</td>
                <td class="metric">9.7√ó</td>
                <td>14</td>
            </tr>
            <tr>
                <td>1000-1100</td>
                <td class="metric">79.3%</td>
                <td class="metric">16.5√ó</td>
                <td class="metric">91.5%</td>
                <td class="metric">10.6√ó</td>
                <td>11</td>
            </tr>
            <tr>
                <td>1100-1200</td>
                <td class="metric">76.4%</td>
                <td class="metric">17.7√ó</td>
                <td class="metric">89.8%</td>
                <td class="metric">11.3√ó</td>
                <td>8</td>
            </tr>
            <tr>
                <td>1200-1300</td>
                <td class="metric">59.1%</td>
                <td class="metric">19.7√ó</td>
                <td class="metric">87.1%</td>
                <td class="metric">12.6√ó</td>
                <td>4</td>
            </tr>
        </tbody>
    </table>

    <h3>Why Performance Degrades Beyond 10√ó</h3>
    <ol>
        <li><strong>Layout Complexity:</strong> Longer documents tend to have more complex layouts with multiple columns, tables, mixed fonts</li>
        <li><strong>Visual Resolution Limits:</strong> At 512√ó512 or 640√ó640, 1200+ character documents become visually blurry‚Äîeven humans struggle to read them</li>
        <li><strong>Token Capacity Bounds:</strong> 64 vision tokens simply cannot encode all the nuanced variations in 1200 text tokens without information loss</li>
    </ol>

    <h2>Practical Performance: OmniDocBench</h2>

    <h3>Overall Results</h3>
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>Avg Tokens/Page</th>
                <th>Edit Distance</th>
                <th>Performance</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td colspan="4" style="background-color: #f0f0f0; font-weight: bold;">Pipeline Models</td>
            </tr>
            <tr>
                <td>MinerU-2.1.1</td>
                <td class="metric">~6000</td>
                <td class="metric">0.162 (EN)</td>
                <td>High quality, very expensive</td>
            </tr>
            <tr>
                <td>PPstructure-v3</td>
                <td class="metric">~6000</td>
                <td class="metric">0.152 (EN)</td>
                <td>Best pipeline, but token-heavy</td>
            </tr>
            <tr>
                <td colspan="4" style="background-color: #f0f0f0; font-weight: bold;">End-to-End Models (High Token Count)</td>
            </tr>
            <tr>
                <td>InternVL3-78B</td>
                <td class="metric">6790</td>
                <td class="metric">0.218 (EN)</td>
                <td>Excellent but expensive</td>
            </tr>
            <tr>
                <td>Qwen2.5-VL-72B</td>
                <td class="metric">3949</td>
                <td class="metric">0.214 (EN)</td>
                <td>High quality, moderate cost</td>
            </tr>
            <tr>
                <td>MinerU2.0</td>
                <td class="metric">6790</td>
                <td class="metric">0.133 (EN)</td>
                <td>Top performer but token-heavy</td>
            </tr>
            <tr>
                <td colspan="4" style="background-color: #f0f0f0; font-weight: bold;">End-to-End Models (Low Token Count)</td>
            </tr>
            <tr>
                <td>GOT-OCR2.0</td>
                <td class="metric">256</td>
                <td class="metric">0.287 (EN)</td>
                <td>Efficient but lower quality</td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>DeepSeek-OCR (Small)</strong></td>
                <td class="metric"><strong>100</strong></td>
                <td class="metric"><strong>0.221 (EN)</strong></td>
                <td class="performance-improvement"><strong>Beats GOT with 2.5√ó fewer tokens!</strong></td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>DeepSeek-OCR (Large)</strong></td>
                <td class="metric"><strong>400 (285 valid)</strong></td>
                <td class="metric"><strong>0.138 (EN)</strong></td>
                <td class="performance-improvement"><strong>Matches top models, 15√ó fewer tokens</strong></td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>DeepSeek-OCR (Gundam)</strong></td>
                <td class="metric"><strong>795</strong></td>
                <td class="metric"><strong>0.127 (EN)</strong></td>
                <td class="performance-improvement"><strong>Beats MinerU2.0 with 8.5√ó fewer tokens!</strong></td>
            </tr>
        </tbody>
    </table>

    <h3>Per-Document-Type Analysis</h3>
    <table>
        <thead>
            <tr>
                <th>Document Type</th>
                <th>Tiny (64)</th>
                <th>Small (100)</th>
                <th>Base (256)</th>
                <th>Gundam</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Slides</strong></td>
                <td class="metric">0.116</td>
                <td class="metric">0.111</td>
                <td class="metric">0.080</td>
                <td class="metric">0.085</td>
            </tr>
            <tr>
                <td><strong>Books</strong></td>
                <td class="metric">0.147</td>
                <td class="metric">0.085</td>
                <td class="metric">0.037</td>
                <td class="metric">0.035</td>
            </tr>
            <tr>
                <td><strong>Financial Reports</strong></td>
                <td class="metric">0.207</td>
                <td class="metric">0.079</td>
                <td class="metric">0.027</td>
                <td class="metric">0.289</td>
            </tr>
            <tr>
                <td><strong>Academic Papers</strong></td>
                <td class="metric">0.395</td>
                <td class="metric">0.131</td>
                <td class="metric">0.052</td>
                <td class="metric">0.039</td>
            </tr>
            <tr>
                <td><strong>Newspapers</strong></td>
                <td class="metric">0.940</td>
                <td class="metric">0.744</td>
                <td class="metric">0.645</td>
                <td class="metric">0.122</td>
            </tr>
        </tbody>
    </table>

    <div class="key-finding">
        <h4>Insights from Per-Type Performance</h4>
        <ul>
            <li><strong>Slides & Books:</strong> Excellent with just 100 tokens (7-10√ó compression)‚Äîmost contain < 1000 text tokens</li>
            <li><strong>Financial Reports & Academic Papers:</strong> Need 256+ tokens for complex layouts and mixed content</li>
            <li><strong>Newspapers:</strong> Require Gundam mode (4000-5000 text tokens, need lower compression ratio)</li>
        </ul>
        <p><strong>Practical Implication:</strong> Different document types have different optimal compression ratios. Adaptive token allocation based on document type can optimize cost-performance tradeoff.</p>
    </div>

    <h2>Advanced Capabilities: "Deep Parsing"</h2>

    <h3>OCR 2.0: Beyond Text Recognition</h3>
    
    <div class="eli5-box">
        <h3>üé® What is Deep Parsing?</h3>
        <p>Imagine you're scanning a science textbook. Regular OCR just reads the words. But what about:</p>
        <ul>
            <li>üìä <strong>Charts and graphs:</strong> Should extract the actual data, not just "there's a chart here"</li>
            <li>üß™ <strong>Chemical formulas:</strong> Should convert to computer-readable format (SMILES)</li>
            <li>üìê <strong>Geometry diagrams:</strong> Should describe the shapes, lines, angles</li>
            <li>üñºÔ∏è <strong>Photos:</strong> Should describe what's in the image</li>
        </ul>
        <p><strong>DeepSeek-OCR does all of this automatically!</strong> One unified prompt, and it figures out what type of content it's looking at and provides the appropriate structured output.</p>
    </div>

    <h3>Deep Parsing Capabilities</h3>
    <table>
        <thead>
            <tr>
                <th>Content Type</th>
                <th>Input</th>
                <th>Output Format</th>
                <th>Applications</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Charts</strong></td>
                <td>Line, bar, pie, composite charts</td>
                <td>HTML table with structured data</td>
                <td>Financial analysis, data extraction from reports</td>
            </tr>
            <tr>
                <td><strong>Chemical Formulas</strong></td>
                <td>Molecular structure images</td>
                <td>SMILES notation</td>
                <td>Chemistry research, drug discovery databases</td>
            </tr>
            <tr>
                <td><strong>Geometry</strong></td>
                <td>Plane geometry figures</td>
                <td>Structured dictionary: segments, coordinates, types</td>
                <td>Math education, geometric reasoning</td>
            </tr>
            <tr>
                <td><strong>Natural Images</strong></td>
                <td>Photos in documents</td>
                <td>Dense caption describing scene</td>
                <td>Document understanding, accessibility</td>
            </tr>
        </tbody>
    </table>

    <h3>Unified Interface: Single Prompt</h3>
    <div class="formula">
        &lt;image&gt;\nParse the figure.
    </div>
    <p>With this single prompt, DeepSeek-OCR automatically:</p>
    <ol>
        <li>Identifies the content type</li>
        <li>Selects appropriate parsing strategy</li>
        <li>Returns structured output in the correct format</li>
    </ol>

    <h3>Multilingual Support</h3>
    <ul>
        <li><strong>Languages:</strong> Nearly 100 languages including Chinese, English, Arabic, Sinhala, etc.</li>
        <li><strong>Layout Support:</strong> Both layout-aware and layout-free OCR modes</li>
        <li><strong>Training Strategy:</strong> Model flywheel for minority languages (layout model + self-trained OCR ‚Üí labels for more training)</li>
    </ul>

    <h2>The Memory Forgetting Mechanism</h2>

    <h3>Mimicking Human Memory Decay</h3>
    
    <div class="eli5-box">
        <h3>üß† How Human Memory Works</h3>
        <p><strong>Think about remembering a conversation:</strong></p>
        <ul>
            <li><strong>Just happened (5 seconds ago):</strong> Crystal clear‚Äîyou remember exact words, tone, context</li>
            <li><strong>Recent (1 hour ago):</strong> Very clear‚Äîyou remember the main points and most details</li>
            <li><strong>Today (6 hours ago):</strong> Clear‚Äîyou remember the gist but some details are fuzzy</li>
            <li><strong>Yesterday:</strong> Blurry‚Äîyou remember it happened and the main idea</li>
            <li><strong>Last week:</strong> Very blurry‚Äîjust a vague recollection</li>
            <li><strong>Last year:</strong> Almost gone‚Äîmaybe just "I talked to someone about something"</li>
        </ul>
        <p><strong>DeepSeek-OCR proposes the same idea for AI:</strong> Convert old conversation text to images, then progressively make those images smaller and blurrier as time passes!</p>
    </div>

    <h3>Implementation Concept</h3>
    <div class="methodology-box">
        <h4>Multi-Level Context Compression</h4>
        <p><span class="badge">TEMPORAL DECAY STRATEGY</span></p>
        <ol>
            <li><strong>Current turn (just happened):</strong> Keep as text tokens‚Äîfull fidelity</li>
            <li><strong>Recent history (1-5 turns ago):</strong> Convert to Gundam mode (high resolution) ‚Üí ~10√ó compression</li>
            <li><strong>Medium history (6-20 turns ago):</strong> Downsample to Large mode (1280√ó1280) ‚Üí 15√ó compression</li>
            <li><strong>Older history (21-50 turns ago):</strong> Downsample to Base mode (1024√ó1024) ‚Üí 20√ó compression</li>
            <li><strong>Ancient history (50+ turns ago):</strong> Downsample to Small/Tiny mode (640√ó640 or 512√ó512) ‚Üí 30-40√ó compression</li>
        </ol>

        <h4>Progressive Resolution Degradation</h4>
        <div class="formula">
            Text ‚Üí Image_High_Res ‚Üí Image_Med_Res ‚Üí Image_Low_Res ‚Üí Discard
        </div>
        <p>As conversations age, progressively downsample the rendered images. This mirrors both:</p>
        <ul>
            <li><strong>Temporal decay:</strong> Memories fade over time</li>
            <li><strong>Spatial decay:</strong> Visual perception degrades with distance</li>
        </ul>
    </div>

    <h3>Practical Benefits</h3>
    <table>
        <thead>
            <tr>
                <th>Without Forgetting</th>
                <th>With Optical Forgetting</th>
                <th>Benefit</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>100 turn conversation<br>100,000 tokens total</td>
                <td>Recent: 10,000 tokens (text)<br>Medium: 2,000 tokens (vision)<br>Old: 1,000 tokens (vision)</td>
                <td class="performance-improvement">87% token reduction!</td>
            </tr>
            <tr>
                <td>Quadratic attention cost<br>100,000¬≤ operations</td>
                <td>Linear in compressed form<br>~13,000¬≤ operations</td>
                <td class="performance-improvement">60√ó fewer operations</td>
            </tr>
            <tr>
                <td>Context limit: 100k tokens<br>= ~100 turns max</td>
                <td>Context limit: 100k tokens<br>= ~1000+ turns possible</td>
                <td class="performance-improvement">10√ó longer conversations</td>
            </tr>
        </tbody>
    </table>

    <h3>Research Implications</h3>
    <div class="key-finding">
        <p><strong>This approach suggests a new paradigm for ultra-long context LLMs:</strong></p>
        <ul>
            <li><strong>Theoretically unlimited context:</strong> Old contexts consume progressively fewer tokens</li>
            <li><strong>Biologically inspired:</strong> Mimics human memory's natural forgetting curve</li>
            <li><strong>Information-theoretic optimality:</strong> Allocates representation capacity where it matters most (recent context)</li>
            <li><strong>Computational efficiency:</strong> Dramatic reduction in quadratic attention costs</li>
        </ul>
        <p><strong>Open Question:</strong> Can LLMs be pretrained with digital-optical text interleaving to natively support this compression mechanism?</p>
    </div>

    <h2>Comparison with Existing Systems</h2>

    <h3>End-to-End OCR Models</h3>
    <table>
        <thead>
            <tr>
                <th>Model</th>
                <th>Tokens/Page</th>
                <th>Strengths</th>
                <th>Limitations</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Nougat</strong></td>
                <td>2352</td>
                <td>First academic paper OCR, pioneering work</td>
                <td>Huge token count, limited to academic papers</td>
            </tr>
            <tr>
                <td><strong>GOT-OCR2.0</strong></td>
                <td>256</td>
                <td>Efficient, supports OCR 2.0 tasks</td>
                <td>Lower accuracy than pipeline methods</td>
            </tr>
            <tr>
                <td><strong>Qwen2.5-VL-72B</strong></td>
                <td>3949</td>
                <td>High quality, general VLM</td>
                <td>Token-heavy, expensive inference</td>
            </tr>
            <tr>
                <td><strong>InternVL3-78B</strong></td>
                <td>6790</td>
                <td>Excellent quality, handles extreme resolutions</td>
                <td>Excessive fragmentation, very expensive</td>
            </tr>
            <tr>
                <td><strong>MinerU2.0</strong></td>
                <td>6790</td>
                <td>Top accuracy on OmniDocBench</td>
                <td>Most expensive, 6000+ tokens/page</td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>DeepSeek-OCR</strong></td>
                <td><strong>100-800</strong></td>
                <td><strong>Best accuracy per token, adaptive modes, fast inference</strong></td>
                <td><strong>Not a chatbot (no SFT), requires completion prompts</strong></td>
            </tr>
        </tbody>
    </table>

    <h3>Vision Encoders in VLMs</h3>
    <table>
        <thead>
            <tr>
                <th>Architecture</th>
                <th>Example</th>
                <th>Problem</th>
                <th>DeepEncoder Advantage</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Dual-Tower</strong></td>
                <td>Vary, DeepSeek-VL</td>
                <td>Complex preprocessing, hard to parallelize</td>
                <td>Single serial pipeline, simple deployment</td>
            </tr>
            <tr>
                <td><strong>Tile-Based</strong></td>
                <td>InternVL2/3</td>
                <td>Low native resolution ‚Üí excessive fragmentation ‚Üí too many tokens</td>
                <td>High native resolution (1024+) ‚Üí minimal tiling, fewer tokens</td>
            </tr>
            <tr>
                <td><strong>Adaptive Resolution</strong></td>
                <td>Qwen2-VL, NaViT</td>
                <td>Massive activation memory ‚Üí GPU OOM on large images</td>
                <td>Window attention + compression ‚Üí manageable memory</td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>Serial Compression</strong></td>
                <td><strong>DeepEncoder</strong></td>
                <td><strong>‚Äî</strong></td>
                <td><strong>Combines benefits: high resolution + low activation + few tokens</strong></td>
            </tr>
        </tbody>
    </table>

    <h3>Unique Positioning</h3>
    <div class="key-finding">
        <h4>DeepSeek-OCR fills a critical gap:</h4>
        <ul>
            <li><strong>vs Pipeline Models:</strong> End-to-end, no separate detection/recognition steps, faster inference</li>
            <li><strong>vs General VLMs:</strong> Optimized for OCR with extreme token efficiency, 5-10√ó fewer tokens</li>
            <li><strong>vs Existing End-to-End OCR:</strong> Better accuracy-per-token ratio, adaptive compression</li>
            <li><strong>vs Research Systems:</strong> Production-ready (200k+ pages/day on single A100), open-source</li>
        </ul>
    </div>

    <h2>Production Deployment</h2>

    <h3>Performance Characteristics</h3>
    <table>
        <thead>
            <tr>
                <th>Metric</th>
                <th>Value</th>
                <th>Notes</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Throughput</strong></td>
                <td class="metric">200,000+ pages/day</td>
                <td>Single A100-40G GPU</td>
            </tr>
            <tr>
                <td><strong>Cluster Scale</strong></td>
                <td class="metric">33M pages/day</td>
                <td>20 nodes √ó 8 A100s (160 GPUs)</td>
            </tr>
            <tr>
                <td><strong>Inference Speed</strong></td>
                <td class="metric">~9 pages/second</td>
                <td>Base mode (256 tokens)</td>
            </tr>
            <tr>
                <td><strong>Memory per Image</strong></td>
                <td class="metric">~2-4GB peak</td>
                <td>Depends on resolution mode</td>
            </tr>
            <tr>
                <td><strong>Model Size</strong></td>
                <td class="metric">3.4GB</td>
                <td>380M encoder + 570M active decoder</td>
            </tr>
        </tbody>
    </table>

    <h3>Cost-Performance Analysis</h3>
    <div class="methodology-box">
        <h4>Comparison: DeepSeek-OCR vs Traditional Pipelines</h4>
        <p><strong>Traditional Pipeline (MinerU2.0):</strong></p>
        <ul>
            <li>Detection model: YOLOv8/layout model ‚Üí 100ms/page</li>
            <li>Recognition model: Multiple OCR calls ‚Üí 300-500ms/page</li>
            <li>Output: 6000+ tokens/page</li>
            <li>Total: ~600ms/page, high token cost</li>
        </ul>

        <p><strong>DeepSeek-OCR (Base mode):</strong></p>
        <ul>
            <li>Single forward pass: ~110ms/page</li>
            <li>Output: 256 tokens/page (182 valid)</li>
            <li>Accuracy: Comparable to MinerU2.0</li>
            <li>Advantage: 5√ó faster, 30√ó fewer tokens</li>
        </ul>
    </div>

    <h3>Use Cases</h3>
    <ol>
        <li><strong>LLM/VLM Training Data Generation:</strong> Convert large PDF corpora to structured text at scale</li>
        <li><strong>Document Search Indexing:</strong> Extract searchable text from scanned documents</li>
        <li><strong>Archival Digitization:</strong> Process historical documents, newspapers, books</li>
        <li><strong>Scientific Literature Processing:</strong> Extract text, formulas, charts from research papers</li>
        <li><strong>Financial Document Analysis:</strong> Parse reports, extract structured data from charts</li>
    </ol>

    <h2>Technical Innovations Explained</h2>

    <h3>Why Window Attention + Global Attention Works</h3>
    
    <div class="eli5-box">
        <h3>üîç The Two-Stage Processing Trick</h3>
        <p><strong>The Problem:</strong> Looking at a high-resolution image with global attention (where every pixel attends to every other pixel) requires MASSIVE memory.</p>
        <p>For a 1024√ó1024 image = 1 million pixels:<br>
        Global attention needs: 1M √ó 1M = 1 trillion operations! ü§Ø</p>
        
        <p><strong>The Solution:</strong> Split the work into two stages:</p>
        <ol>
            <li><strong>Stage 1 (Window Attention):</strong> Look at small 16√ó16 windows independently. Each window only attends to itself.
                <ul>
                    <li>Operations: 256 √ó 256 √ó (1M/256) = 256 million (1000√ó less!)</li>
                    <li>This stage captures LOCAL details (edges, characters, small patterns)</li>
                </ul>
            </li>
            <li><strong>Compression:</strong> Squish all those windows down 16√ó‚Äînow you have only 4096 tokens</li>
            <li><strong>Stage 2 (Global Attention):</strong> Now use global attention on the compressed tokens.
                <ul>
                    <li>Operations: 4096 √ó 4096 = 16 million (still manageable!)</li>
                    <li>This stage captures GLOBAL context (layout, relationships, meaning)</li>
                </ul>
            </li>
        </ol>
        <p><strong>Result:</strong> You get both local details AND global understanding, without exploding memory!</p>
    </div>

    <h3>Position Encoding for Variable Resolution</h3>
    <p><strong>The Challenge:</strong> The model is trained on specific resolutions (512, 640, 1024, 1280). How does it handle arbitrary sizes?</p>
    
    <p><strong>The Solution: Dynamic Positional Encoding Interpolation</strong></p>
    <div class="formula">
        PE(pos) = sin(pos / 10000^(2i/d))<br>
        For new resolution: Interpolate between learned positions
    </div>
    <ul>
        <li>If image is 800√ó800 (between 640 and 1024), interpolate position embeddings</li>
        <li>SAM and CLIP both support this via careful architectural design</li>
        <li>Enables smooth handling of any resolution without retraining</li>
    </ul>

    <h3>Why MoE for the Decoder?</h3>
    <div class="methodology-box">
        <h4>Mixture-of-Experts Benefits</h4>
        <p><strong>Traditional Decoder:</strong> Every token activates entire model ‚Üí expensive</p>
        <p><strong>MoE Decoder:</strong> Each token routes to 6 of 64 experts ‚Üí only activates small portion</p>
        
        <table>
            <thead>
                <tr>
                    <th>Aspect</th>
                    <th>Dense 3B Model</th>
                    <th>DeepSeek3B-MoE</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Total Parameters</td>
                    <td>3B</td>
                    <td>3B (64 experts √ó 45M each)</td>
                </tr>
                <tr>
                    <td>Active per Token</td>
                    <td>3B</td>
                    <td>570M (6 routed + 2 shared)</td>
                </tr>
                <tr>
                    <td>FLOPs</td>
                    <td>High</td>
                    <td class="performance-improvement">5√ó lower</td>
                </tr>
                <tr>
                    <td>Expressivity</td>
                    <td>Baseline</td>
                    <td class="performance-improvement">Higher (64 specialized sub-models)</td>
                </tr>
            </tbody>
        </table>
        
        <p><strong>Why This Matters for OCR:</strong></p>
        <ul>
            <li>Different experts specialize: text recognition, layout understanding, formula parsing, etc.</li>
            <li>Fast inference critical for 200k pages/day throughput</li>
            <li>Can fit larger total capacity on single GPU by activating small portion</li>
        </ul>
    </div>

    <h3>The "Model Flywheel" for Minority Languages</h3>
    <div class="key-finding">
        <h4>Bootstrapping OCR for 100 Languages</h4>
        <p><strong>Problem:</strong> No labeled OCR data for minority languages (Sinhala, Khmer, etc.)</p>
        <p><strong>Solution: Iterative Self-Improvement</strong></p>
        <ol>
            <li><strong>Step 1:</strong> Use PP-DocLayout (layout model) ‚Üí finds text regions (works across languages)</li>
            <li><strong>Step 2:</strong> Use fitz to extract raw text ‚Üí creates initial training data</li>
            <li><strong>Step 3:</strong> Train GOT-OCR2.0 on this noisy data ‚Üí gets basic OCR ability</li>
            <li><strong>Step 4:</strong> Use trained model to label more documents ‚Üí creates better training data</li>
            <li><strong>Step 5:</strong> Retrain on improved data ‚Üí model gets better</li>
            <li><strong>Repeat steps 4-5:</strong> Model quality improves with each iteration</li>
        </ol>
        <p><strong>Result:</strong> 600K labeled samples for minority languages, good enough for production use</p>
    </div>

    <h2>Critique & Limitations</h2>

    <h3>Current Limitations</h3>
    <ol>
        <li><strong>No Supervised Fine-Tuning (SFT):</strong>
            <ul>
                <li>Model is not a chatbot‚Äîrequires completion-style prompts</li>
                <li>Cannot follow complex multi-step instructions like general VLMs</li>
                <li>Less user-friendly than conversational models</li>
                <li><strong>Impact:</strong> Limits adoption for non-technical users</li>
            </ul>
        </li>
        <li><strong>Unproven for True Context Compression:</strong>
            <ul>
                <li>Only tested on OCR tasks (vision‚Üítext)</li>
                <li>Digital-optical text interleaving not yet validated</li>
                <li>Needle-in-haystack tests on compressed contexts needed</li>
                <li><strong>Impact:</strong> Memory forgetting mechanism remains theoretical</li>
            </ul>
        </li>
        <li><strong>Geometry Parsing Still Challenging:</strong>
            <ul>
                <li>Interdependent line segments create complex structured output</li>
                <li>Accuracy lower than other OCR 2.0 tasks (charts, formulas)</li>
                <li><strong>Impact:</strong> Limited use for mathematical diagram understanding</li>
            </ul>
        </li>
        <li><strong>Compression-Accuracy Tradeoff:</strong>
            <ul>
                <li>Performance degrades significantly beyond 10√ó compression</li>
                <li>No adaptive mechanism to allocate more tokens to complex regions</li>
                <li><strong>Impact:</strong> Cannot handle very long, complex documents with tiny token budgets</li>
            </ul>
        </li>
        <li><strong>Limited to Static Documents:</strong>
            <ul>
                <li>Cannot handle video, animations, interactive content</li>
                <li>No temporal modeling for sequential visual information</li>
                <li><strong>Impact:</strong> Narrow applicability compared to general VLMs</li>
            </ul>
        </li>
    </ol>

    <h3>Architectural Concerns</h3>
    <ul>
        <li><strong>SAM + CLIP Coupling:</strong> Requires maintaining two separate pretrained models, increases deployment complexity</li>
        <li><strong>Fixed Compression Ratio:</strong> 16√ó compressor is hard-coded, cannot adapt based on image content</li>
        <li><strong>No Learned Downsampling:</strong> Convolutional compressor is relatively simple, might miss semantic information</li>
        <li><strong>Single-Pass Encoding:</strong> Cannot iteratively refine vision tokens based on decoder needs</li>
    </ul>

    <h2>Future Research Directions</h2>

    <h3>Short-Term Improvements (6-12 months)</h3>
    <ol>
        <li><strong>Add Supervised Fine-Tuning Stage:</strong>
            <ul>
                <li>Make model conversational for better UX</li>
                <li>Add instruction-following capabilities</li>
                <li>Support for multi-turn document Q&A</li>
            </ul>
        </li>
        <li><strong>Adaptive Compression:</strong>
            <ul>
                <li>Learn to allocate more tokens to complex regions (dense tables, formulas)</li>
                <li>Content-aware compression ratio selection</li>
                <li>Region-based quality-token tradeoff</li>
            </ul>
        </li>
        <li><strong>Improved Geometry Parsing:</strong>
            <ul>
                <li>Graph neural networks for relational structure</li>
                <li>Constraint satisfaction for geometric consistency</li>
                <li>Benchmark on GeoQA, UniGeo datasets</li>
            </ul>
        </li>
        <li><strong>Streaming Inference:</strong>
            <ul>
                <li>Process documents in chunks for memory efficiency</li>
                <li>Progressive token generation as image processes</li>
                <li>Reduce latency for large documents</li>
            </ul>
        </li>
    </ol>

    <h3>Medium-Term Exploration (1-2 years)</h3>
    <ol>
        <li><strong>Digital-Optical Interleaved Pretraining:</strong>
            <ul>
                <li>Train LLMs from scratch with mixed text and rendered-text-as-image</li>
                <li>Test if models learn natural compression/decompression</li>
                <li>Validate needle-in-haystack on compressed contexts</li>
                <li><strong>Key Question:</strong> Can LLMs natively develop optical compression during pretraining?</li>
            </ul>
        </li>
        <li><strong>Learned Forgetting Mechanisms:</strong>
            <ul>
                <li>Train models to decide WHAT to compress (importance-based)</li>
                <li>Learn compression schedules (HOW MUCH to compress over time)</li>
                <li>Implement retrieval-based "refreshing" of important old memories</li>
            </ul>
        </li>
        <li><strong>Hierarchical Vision Tokenization:</strong>
            <ul>
                <li>Multi-scale token representations (coarse to fine)</li>
                <li>Decoder can attend to different resolution levels as needed</li>
                <li>Adaptive compute allocation</li>
            </ul>
        </li>
        <li><strong>Cross-Modal Compression:</strong>
            <ul>
                <li>Extend to audio‚Üítext compression (transcribe+summarize)</li>
                <li>Video‚Üítext compression (frame sampling + OCR + tracking)</li>
                <li>Unified compression framework across modalities</li>
            </ul>
        </li>
    </ol>

    <h3>Long-Term Vision (2+ years)</h3>
    <ol>
        <li><strong>Lossless Compression Bounds:</strong>
            <ul>
                <li>Theoretical analysis: what is maximum lossless compression ratio?</li>
                <li>Information-theoretic limits for different content types</li>
                <li>Optimal allocation strategies</li>
            </ul>
        </li>
        <li><strong>Multi-Agent Collaborative Compression:</strong>
            <ul>
                <li>Specialist encoder per content type (text, math, charts)</li>
                <li>Routing mechanism selects appropriate encoder</li>
                <li>Ensemble compression for mixed-content documents</li>
            </ul>
        </li>
        <li><strong>Real-Time Adaptive Systems:</strong>
            <ul>
                <li>Live compression quality adjustment based on downstream task performance</li>
                <li>Reinforcement learning to optimize compression-accuracy tradeoff</li>
                <li>Meta-learning for rapid adaptation to new document types</li>
            </ul>
        </li>
        <li><strong>Unified Multimodal Memory Architecture:</strong>
            <ul>
                <li>Single memory system handling text, vision, audio, code</li>
                <li>Cross-modal compression (summarize conversation as diagram)</li>
                <li>Holistic forgetting mechanisms across modalities</li>
            </ul>
        </li>
    </ol>

    <h2>Novel Research Questions Opened</h2>

    <div class="methodology-box">
        <h3>Fundamental Questions</h3>
        <ol>
            <li><strong>Compression-Understanding Paradox:</strong> If text is compressed 10√ó, has the model truly "understood" it, or just memorized a lookup table? How do we distinguish compression from understanding?</li>
            <li><strong>Optimal Rendering Strategies:</strong> Is standard text rendering optimal, or should we design special "compression-friendly" fonts/layouts that maximize information density?</li>
            <li><strong>Cross-Lingual Compression:</strong> Do different languages compress differently when rendered optically? Should compression strategies vary by language?</li>
            <li><strong>Semantic Preservation:</strong> At what compression ratio do semantics degrade? Can we prioritize semantic information over syntactic details?</li>
            <li><strong>Temporal vs Spatial Compression:</strong> The paper suggests time-based forgetting mimics spatial distance. Are these truly analogous, or do they require different mechanisms?</li>
        </ol>
    </div>

    <h2>Implementation Insights for Practitioners</h2>

    <h3>When to Use DeepSeek-OCR</h3>
    <div class="methodology-box">
        <h4>‚úÖ Ideal Use Cases</h4>
        <ul>
            <li><strong>Large-scale document processing:</strong> Converting millions of PDFs to structured text</li>
            <li><strong>Training data generation:</strong> Creating LLM pretraining corpora from scanned documents</li>
            <li><strong>Cost-sensitive applications:</strong> Where token efficiency directly impacts costs</li>
            <li><strong>High-throughput scenarios:</strong> Need to process 100k+ pages/day</li>
            <li><strong>Multilingual documents:</strong> Content in diverse languages including minority languages</li>
            <li><strong>Mixed content:</strong> Documents with text, charts, formulas, geometry</li>
        </ul>

        <h4>‚ùå When to Use Alternatives</h4>
        <ul>
            <li><strong>Chatbot interfaces:</strong> Need conversational interaction ‚Üí use Qwen2.5-VL, InternVL3</li>
            <li><strong>Complex reasoning:</strong> Multi-step analysis over documents ‚Üí use GPT-4V, Gemini Pro</li>
            <li><strong>Maximum accuracy critical:</strong> Legal/medical where errors unacceptable ‚Üí use MinerU2.0</li>
            <li><strong>Interactive applications:</strong> Real-time user feedback, iterative refinement ‚Üí use general VLMs</li>
            <li><strong>General vision understanding:</strong> Need broad capabilities beyond OCR ‚Üí use foundation VLMs</li>
        </ul>
    </div>

    <h3>Deployment Considerations</h3>
    <table>
        <thead>
            <tr>
                <th>Aspect</th>
                <th>Recommendation</th>
                <th>Rationale</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>GPU Requirements</strong></td>
                <td>A100-40G or better</td>
                <td>Base model fits, Gundam mode needs memory headroom</td>
            </tr>
            <tr>
                <td><strong>Batch Size</strong></td>
                <td>16-32 images</td>
                <td>Balance throughput vs memory</td>
            </tr>
            <tr>
                <td><strong>Mode Selection</strong></td>
                <td>Start with Small (100 tokens)</td>
                <td>Best accuracy-cost tradeoff for most documents</td>
            </tr>
            <tr>
                <td><strong>Prompt Engineering</strong></td>
                <td>Use completion-style prompts</td>
                <td>Model not fine-tuned for instruction following</td>
            </tr>
            <tr>
                <td><strong>Error Handling</strong></td>
                <td>Retry with higher resolution on failures</td>
                <td>Adaptive quality based on content complexity</td>
            </tr>
            <tr>
                <td><strong>Preprocessing</strong></td>
                <td>Convert PDFs at 200 DPI</td>
                <td>Optimal quality-size tradeoff per paper</td>
            </tr>
        </tbody>
    </table>

    <h3>Cost-Benefit Analysis</h3>
    <div class="key-finding">
        <h4>Example: Processing 1M Pages</h4>
        <table>
            <thead>
                <tr>
                    <th>Approach</th>
                    <th>GPU Hours</th>
                    <th>Token Cost</th>
                    <th>Quality</th>
                    <th>Total Cost</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>GPT-4V API</strong></td>
                    <td>‚Äî</td>
                    <td>$20,000</td>
                    <td>Excellent</td>
                    <td class="metric">$20,000</td>
                </tr>
                <tr>
                    <td><strong>InternVL3-78B</strong></td>
                    <td>5000</td>
                    <td>‚Äî</td>
                    <td>Excellent</td>
                    <td class="metric">$15,000</td>
                </tr>
                <tr>
                    <td><strong>MinerU2.0</strong></td>
                    <td>3000</td>
                    <td>‚Äî</td>
                    <td>Best</td>
                    <td class="metric">$9,000</td>
                </tr>
                <tr style="background-color: #fff8f0;">
                    <td><strong>DeepSeek-OCR (Small)</strong></td>
                    <td><strong>200</strong></td>
                    <td>‚Äî</td>
                    <td><strong>Very Good</strong></td>
                    <td class="performance-improvement"><strong>$600</strong></td>
                </tr>
            </tbody>
        </table>
        <p><strong>Assuming:</strong> A100 at $3/hour, GPT-4V at $0.02/page for OCR-length content</p>
        <p><strong>Result:</strong> DeepSeek-OCR is 15-30√ó cheaper while maintaining high quality</p>
    </div>

    <h2>Connections to Broader AI Research</h2>

    <h3>Relation to Other Compression Techniques</h3>
    <table>
        <thead>
            <tr>
                <th>Technique</th>
                <th>Mechanism</th>
                <th>Compression</th>
                <th>Lossiness</th>
                <th>Relation to DeepSeek-OCR</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td><strong>Token Pruning</strong></td>
                <td>Remove redundant tokens</td>
                <td>2-5√ó</td>
                <td>Low</td>
                <td>Complementary‚Äîcould prune before optical compression</td>
            </tr>
            <tr>
                <td><strong>KV Cache Compression</strong></td>
                <td>Compress attention cache</td>
                <td>2-4√ó</td>
                <td>Medium</td>
                <td>Orthogonal‚Äîapplies during inference, not context encoding</td>
            </tr>
            <tr>
                <td><strong>Summarization</strong></td>
                <td>LLM rewrites shorter</td>
                <td>3-10√ó</td>
                <td>High</td>
                <td>Similar goal, but optical preserves visual structure</td>
            </tr>
            <tr>
                <td><strong>Retrieval</strong></td>
                <td>Store externally, fetch</td>
                <td>‚àû</td>
                <td>None</td>
                <td>Could store old contexts as images for retrieval</td>
            </tr>
            <tr style="background-color: #fff8f0;">
                <td><strong>Optical Compression</strong></td>
                <td><strong>Render as image</strong></td>
                <td><strong>7-20√ó</strong></td>
                <td><strong>Low-High</strong></td>
                <td><strong>Novel modality-crossing approach</strong></td>
            </tr>
        </tbody>
    </table>

    <h3>Implications for Multimodal Foundation Models</h3>
    <div class="key-finding">
        <p><strong>DeepSeek-OCR suggests a paradigm shift in multimodal model design:</strong></p>
        <ol>
            <li><strong>Vision as Infrastructure, Not Feature:</strong> Vision encoders should be optimized for text processing efficiency, not just image understanding</li>
            <li><strong>Modality-Crossing Compression:</strong> Transform data into most efficient modality for representation (text‚Üíimage‚Üícompressed text)</li>
            <li><strong>Heterogeneous Token Budgets:</strong> Different parts of context can use different modalities based on age/importance</li>
            <li><strong>Unified Attention Across Modalities:</strong> LLMs attend to both text tokens and vision tokens representing compressed text</li>
        </ol>
    </div>

    <h3>Memory Systems in AI</h3>
    <p>DeepSeek-OCR connects to broader work on memory architectures:</p>
    <ul>
        <li><strong>Sparse Memory (Memorizing Transformers):</strong> Store all past activations, retrieve relevant ones ‚Üí DeepSeek stores as compressed images instead</li>
        <li><strong>Hierarchical Memory (HTM):</strong> Multiple resolution levels ‚Üí similar to multi-resolution optical compression</li>
        <li><strong>Episodic vs Semantic Memory:</strong> Recent contexts (episodic) in high fidelity, old contexts (semantic) compressed to gist</li>
        <li><strong>Working Memory Limits:</strong> Humans have ~7 item working memory ‚Üí corresponds to keeping recent turns as text, compressing older ones</li>
    </ul>

    <h2>Additional Comments</h2>

    <h3>Why This Paper Matters</h3>
    <ol>
        <li><strong>Paradigm Shift:</strong> First work to systematically treat vision encoding as compression mechanism for text processing</li>
        <li><strong>Quantitative Bounds:</strong> Establishes empirical compression-accuracy tradeoffs with clear experimental validation</li>
        <li><strong>Production Viability:</strong> Not just a research prototype‚Äîdeployed system processing millions of pages</li>
        <li><strong>Biological Inspiration:</strong> Memory forgetting mechanism mirrors neuroscience findings on memory consolidation</li>
        <li><strong>Open Source:</strong> Code and weights available, enabling reproducible research and practical deployment</li>
    </ol>

    <h3>Surprising Findings</h3>
    <ul>
        <li><strong>Compression Headroom:</strong> Expected 5-7√ó compression, achieved 10√ó near-lossless‚Äîbetter than anticipated</li>
        <li><strong>Graceful Degradation:</strong> Even at 20√ó compression, 60% accuracy suggests soft failure rather than catastrophic collapse</li>
        <li><strong>Document Type Variance:</strong> Massive spread (slides vs newspapers) indicates compression is content-dependent</li>
        <li><strong>Small Model Sufficiency:</strong> 570M active parameters can decode 10√ó compressed text‚Äîsuggests compression is learnable by modest models</li>
    </ul>

    <h3>Underexplored Aspects in Paper</h3>
    <ol>
        <li><strong>Semantic vs Syntactic Preservation:</strong> Does compression preserve meaning better than exact text? No analysis of semantic similarity metrics</li>
        <li><strong>Multimodal Pretraining Analysis:</strong> What happens if LLMs see compressed text during pretraining? Would they naturally develop decompression abilities?</li>
        <li><strong>Compression Artifacts:</strong> What types of errors occur at different compression ratios? Character-level, word-level, sentence-level?</li>
        <li><strong>Cross-Language Transfer:</strong> Does OCR ability on English transfer to unseen languages? Few-shot adaptation?</li>
        <li><strong>Adversarial Robustness:</strong> Can carefully designed documents "fool" the compression, forcing token allocation to irrelevant content?</li>
    </ol>

    <div class="conclusion-box">
        <h2>Conclusions</h2>
        <p>DeepSeek-OCR represents a significant conceptual advance in addressing long-context challenges in large language models through a novel paradigm: treating vision as a compression medium for text. Key takeaways:</p>

        <h3>Core Contributions</h3>
        <ul>
            <li><strong>Empirical Compression Bounds:</strong> Demonstrates 7-20√ó text compression via optical mapping with quantified accuracy tradeoffs (97% at 10√ó, 60% at 20√ó)</li>
            <li><strong>Novel Architecture Design:</strong> DeepEncoder's serial connection of window attention and global attention achieves simultaneous high resolution, low activation, and few tokens</li>
            <li><strong>Production Deployment:</strong> 200k+ pages/day throughput on single A100 with state-of-the-art accuracy-per-token ratio on OmniDocBench</li>
            <li><strong>Memory Forgetting Framework:</strong> Conceptual foundation for progressive context compression mimicking biological memory decay</li>
        </ul>

        <h3>Paradigm Implications</h3>
        <ul>
            <li><strong>Vision as Infrastructure:</strong> Reframes vision encoders from "image understanding tools" to "text processing accelerators"</li>
            <li><strong>Modality-Crossing Efficiency:</strong> Demonstrates that optimal representation may require transforming between modalities</li>
            <li><strong>Heterogeneous Token Budgets:</strong> Different context segments can use different representations based on recency/importance</li>
            <li><strong>Biologically-Inspired Design:</strong> Forgetting mechanisms that mirror human memory consolidation and decay</li>
        </ul>

        <h3>For Researchers</h3>
        <ol>
            <li><strong>Immediate Exploration:</strong> Digital-optical interleaved pretraining to validate native compression abilities in LLMs</li>
            <li><strong>Architecture Research:</strong> Adaptive compression ratios, content-aware token allocation, hierarchical representations</li>
            <li><strong>Theoretical Analysis:</strong> Information-theoretic limits of optical compression, semantic preservation bounds</li>
            <li><strong>Applications Beyond OCR:</strong> Audio compression, video summarization, code compression</li>
        </ol>

        <h3>For Practitioners</h3>
        <ol>
            <li><strong>Immediate Deployment:</strong> Use for large-scale document processing, training data generation (15-30√ó cost savings vs alternatives)</li>
            <li><strong>Mode Selection Strategy:</strong> Start with Small mode (100 tokens), scale up only when accuracy insufficient</li>
            <li><strong>Integration Patterns:</strong> Combine with retrieval systems (store contexts as compressed images), implement adaptive quality based on downstream task needs</li>
            <li><strong>Future-Proofing:</strong> Design systems with compression-friendly memory architectures anticipating next-gen LLMs with native optical compression</li>
        </ol>

        <h3>Open Questions</h3>
        <ul>
            <li><strong>Theoretical:</strong> What are information-theoretic limits of lossless optical compression? Can we prove bounds?</li>
            <li><strong>Empirical:</strong> Do LLMs pretrained with optical compression generalize better to long contexts? Needle-in-haystack performance?</li>
            <li><strong>Architectural:</strong> Can we learn end-to-end compression policies rather than fixed 16√ó ratios? Dynamic allocation?</li>
            <li><strong>Practical:</strong> How do humans perceive/validate compressed contexts? Is 60% accuracy at 20√ó compression "useful"?</li>
        </ul>

        <h3>Broader Impact</h3>
        <p>DeepSeek-OCR opens pathways toward scalable ultra-long context processing without proportional computational cost increases. By establishing vision-text compression as a viable paradigm and demonstrating production viability, it challenges assumptions about modality separation in foundation models. The work suggests that optimal AI systems may not cleanly separate vision and language processing, but instead fluidly transform between modalities to optimize computational efficiency.</p>

        <p><strong>Most Importantly:</strong> This is early-stage work with substantial room for improvement. The 10√ó near-lossless compression achieved is just the beginning. With proper pretraining integration, learned compression policies, and adaptive mechanisms, future systems might achieve 20-50√ó compression while maintaining high fidelity‚Äîfundamentally changing how we think about context windows in AI.</p>

        <p>The question is no longer "can we compress contexts optically?" but rather "how much further can this paradigm be pushed?" The answer will shape the next generation of multimodal foundation models.</p>
    </div>

    <h2>References</h2>
    <ul>
        <li>Wei, H., Sun, Y., Li, Y. "DeepSeek-OCR: Contexts Optical Compression." arXiv preprint arXiv:2510.18234v1 [cs.CV], October 2025.</li>
        <li>Benchmarks: Fox (Liu et al., 2024), OmniDocBench (Ouyang et al., 2025)</li>
        <li>Architecture Components: SAM (Kirillov et al., 2023), CLIP (Radford et al., 2021), DeepSeekMoE (Liu et al., 2024)</li>
        <li>Comparison Models: GOT-OCR2.0 (Wei et al., 2024), MinerU2.0 (Wang et al., 2024), InternVL3 (Zhu et al., 2025), Qwen2.5-VL (Bai et al., 2025)</li>
        <li>Related VLM Architectures: Vary (Wei et al., 2024), NaViT (Dehghani et al., 2023)</li>
        <li>OCR Pipelines: PaddleOCR (Cui et al., 2025), PP-DocLayout (Sun et al., 2025)</li>
    </ul>

    <div style="margin-top: 50px; padding-top: 20px; border-top: 2px solid #e0e0e0; text-align: center; color: #666; font-size: 12px;">
        <p>Report compiled for AI Agent Engineering Research Collection</p>
        <p>For more resources, visit <a href="https://join.maxpool.dev" style="color: #DC8850;">join.maxpool.dev</a></p>
    </div>
    <div class="navigation">
        <a href="../index.html">‚Üê Home</a>
        <a href="../agent/index.html">Agent Reliability</a>
        <a href="../rag/index.html">RAG Patterns</a>
        <a href="../research-papers/index.html">Research Papers</a>
        <a href="https://join.maxpool.dev" target="_blank">Join Community ‚Üí</a>
    </div>
</body>
</html>