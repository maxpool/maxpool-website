<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Papers - AI Agent Engineering</title>
    <link rel="stylesheet" href="../design-system.css">
    <style>
        .paper-link {
            color: #191817;
            text-decoration: none;
            transition: color 0.2s;
            font-weight: 500;
        }
        .paper-link:hover {
            color: #DC8850;
            text-decoration: underline;
        }
        .pdf-badge {
            background: rgba(220, 136, 80, 0.15);
            color: #DC8850;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.85em;
            margin-left: 8px;
            font-weight: 500;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="main-header">
            <div class="header-content">
                <a href="../" class="nav-home">← Home</a>
                <div class="header-left">
                    <h1 class="heading-2">Research Papers</h1>
                    <div class="subtitle-section">Curated collection of AI agent engineering research and analysis</div>
                </div>
                <div class="cta">
                    <div>
                        <div class="cta-label">GenAI Community</div>
                        <a href="https://join.maxpool.dev" target="_blank" class="cta-link">
                            join.maxpool.dev →
                        </a>
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <div class="section-header">
                <div class="section-title heading-3">Available Papers</div>
                <div class="subtitle-section">Research reports and architectural analysis</div>
            </div>
            <div class="content">
                <div class="table-wrapper">
                    <table>
                        <thead>
                            <tr>
                                <th style="width: 50%">Title</th>
                                <th style="width: 50%">Description</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>
                                    <a href="measuring_agents_production.html" target="_blank" class="paper-link">
                                        <strong>Measuring Agents in Production: Empirical Study of 306 Practitioners</strong>
                                    </a>
                                </td>
                                <td class="description">Landmark empirical study surveying 306 practitioners and 20 in-depth interviews across 26 domains reveals how AI agents actually work in production. Key findings: 68% execute ≤10 steps before human intervention, 70% use off-the-shelf models without fine-tuning, 85% build custom implementations over frameworks, and 74% rely on human evaluation rather than benchmarks. Demonstrates that successful teams deliberately trade capability for controllability—using constrained architectures with predefined workflows (80%) rather than open-ended autonomy. Challenges research assumptions by showing prompting beats tuning, custom code beats frameworks, and reliability engineering beats capability scaling.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="ctm_report.html" target="_blank" class="paper-link">
                                        <strong>Continuous Thought Machines: Neural Synchronization for Emergent Intelligence</strong>
                                    </a>
                                </td>
                                <td class="description">Groundbreaking architecture from Sakana AI treating temporal dynamics as fundamental computation. Introduces Neuron-Level Models (NLMs) giving each neuron private temporal processing, and synchronization matrices capturing neural correlation patterns as core representations. Achieves 6× generalization beyond training on mazes (39×39 → 99×99), near-perfect accuracy on cumulative parity where LSTMs fail, and better-than-human calibration on image classification. Demonstrates emergent adaptive computation (harder problems get more internal ticks), backward attention enabling planning-like behavior, and interpretable "thinking" trajectories—all without explicit design. Bridges computational efficiency with biological plausibility.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="ilya_favorites.html" target="_blank" class="paper-link">
                                        <strong>Ilya's Favorite Papers: A Curated Learning Path for AI</strong>
                                    </a>
                                </td>
                                <td class="description">Comprehensive collection of ~40 foundational papers curated by Ilya Sutskever (former Chief Scientist at OpenAI) as the definitive learning path for understanding modern AI. Covers progression from CNNs and RNNs to Transformers, explores attention mechanisms, memory-augmented networks, and theoretical foundations in information theory and complexity science. Includes detailed summaries and key learnings for each paper, organized thematically from basic concepts to advanced techniques and philosophical foundations.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="ai_reliability_timeline.html" target="_blank" class="paper-link">
                                        <strong>When Will AI Become Reliable? Half-Life Analysis & Long Task Completion</strong>
                                    </a>
                                </td>
                                <td class="description">Synthesis of Toby Ord's half-life framework with METR's exponential growth analysis. Reveals AI agents fail at constant rate per minute (half-life model) while capabilities double every 7 months. Projects specific reliability thresholds: 90% reliability requires 1/7 task duration reduction, current models achieve 50-minute tasks at 50% success. Predicts month-long task automation by 2030, with practical architecture patterns for current reliability levels.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="agent_failure_learning.html" target="_blank" class="paper-link">
                                        <strong>Where LLM Agents Fail and How They Can Learn: Systematic Error Analysis & Remediation</strong>
                                    </a>
                                </td>
                                <td class="description">Groundbreaking framework for understanding agent failures through cascading error analysis. Introduces AgentErrorTaxonomy classifying failures across memory, reflection, planning, action, and system operations. AgentDebug framework achieves 24% higher accuracy and 220% increase in error recovery by identifying root causes and delivering corrective feedback. Demonstrates agents can alter their "constant hazard rate" through principled debugging, with up to 26% relative improvements across ALFWorld, GAIA, and WebShop environments.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="dreamgym_report.html" target="_blank" class="paper-link">
                                        <strong>DreamGym: Scaling Agent Learning via Experience Synthesis</strong>
                                    </a>
                                </td>
                                <td class="description">Breakthrough framework for training AI agents through synthetic experience synthesis. Introduces reasoning-based experience model that simulates environment dynamics, enabling scalable reinforcement learning without costly real-world interactions. Achieves 30%+ improvement on non-RL-ready tasks like WebArena using zero real environment interactions, while matching state-of-the-art on traditional benchmarks. Addresses four critical challenges: costly rollouts, limited task diversity, unreliable rewards, and infrastructure complexity.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="deepseek_ocr_report.html" target="_blank" class="paper-link">
                                        <strong>DeepSeek-OCR: Contexts Optical Compression</strong>
                                    </a>
                                </td>
                                <td class="description">Revolutionary approach treating vision as compression medium for text processing. Introduces DeepEncoder achieving 7-20× text compression with 97% accuracy at 10× ratio through serial window+global attention architecture. Demonstrates 200k+ pages/day throughput while outperforming models using 30× more tokens. Proposes biologically-inspired memory forgetting mechanism via progressive resolution degradation, enabling theoretically unlimited context windows.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="agentflow_report.html" target="_blank" class="paper-link">
                                        <strong>AgentFlow: In-the-Flow Agentic System Optimization for Effective Planning and Tool Use</strong>
                                    </a>
                                </td>
                                <td class="description">Novel trainable agentic framework coordinating four specialized modules (planner, executor, verifier, generator) through evolving memory. Introduces Flow-GRPO training method enabling direct optimization within live multi-turn interactions. Demonstrates 7B models surpassing GPT-4o with 14.9% gains on search tasks, 14.0% on agentic tasks, and 14.5% on mathematical reasoning.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="agentic_context_engineering.html" target="_blank" class="paper-link">
                                        <strong>Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models</strong>
                                    </a>
                                </td>
                                <td class="description">Revolutionary approach treating contexts as "evolving playbooks" that accumulate detailed strategies through generation, reflection, and curation cycles. Introduces incremental delta updates achieving 82.3% reduction in adaptation latency versus GEPA and 83.6% token cost reduction versus Dynamic Cheatsheet. Demonstrates 10.6% improvement on agent tasks and 17.1% on online adaptation challenges. Matches GPT-4.1 production agent performance using smaller open-source models by preventing context collapse and preserving domain-specific insights.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="reasoningbank_report.html" target="_blank" class="paper-link">
                                        <strong>ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory</strong>
                                    </a>
                                </td>
                                <td class="description">Novel memory framework enabling AI agents to learn from both successful and failed experiences by distilling generalizable reasoning strategies. Introduces Memory-aware Test-Time Scaling (MaTTS) that creates synergy between memory quality and computational scaling. Demonstrates up to 34.2% relative improvement across web browsing and software engineering tasks, with emergent self-evolution behaviors.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="curse_of_instructions_report.html" target="_blank" class="paper-link">
                                        <strong>Curse of Instructions: Large Language Models Cannot Follow Multiple Instructions at Once</strong>
                                    </a>
                                </td>
                                <td class="description">Comprehensive analysis revealing fundamental limitations in LLMs' ability to follow multiple simultaneous instructions. Introduces ManyIFEval benchmark showing exponential performance decay with instruction count, with GPT-4o, Claude-3.5, and other models tested. Includes self-refinement mitigation strategies and production implications.</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="benchmarks_critique_report.pdf" target="_blank" class="paper-link">
                                        <strong>AI Benchmark Critique: Evidence of Invalid 2026 Predictions</strong>
                                    </a>
                                </td>
                                <td class="description">Critical analysis of METR and GDPval benchmarks, revealing statistical flaws, baseline inflation errors, and invalid extrapolation methods</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="RSA_Research_Report.pdf" target="_blank" class="paper-link">
                                        <strong>Recursive Self-Aggregation: Deep Thinking and Test-Time Scaling for LLM Reasoning</strong>
                                    </a>
                                </td>
                                <td class="description">Groundbreaking test-time scaling method enabling smaller models to match larger reasoning models through iterative aggregation of reasoning chains</td>
                            </tr>
                            <tr>
                                <td>
                                    <a href="sutton_oak_architecture.pdf" target="_blank" class="paper-link">
                                        <strong>The OaK Architecture: A Paradigm Shift in Artificial General Intelligence</strong>
                                    </a>
                                </td>
                                <td class="description">Rich Sutton's vision for experience-based superintelligence through continual learning, hierarchical abstraction, and reward maximization</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>

        <div class="footer">
            <h3>GenAI Community</h3>
            <p>Join our community for more AI development resources and discussions</p>
            <a href="https://join.maxpool.dev" target="_blank" class="btn-outline">
                Visit join.maxpool.dev →
            </a>
        </div>
    </div>
</body>
</html>
